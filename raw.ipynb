{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(2)\n",
    "b = Tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a*b\n",
    "c.backward()\n",
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 4.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad, a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.zero_grad()\n",
    "a.zero_grad()\n",
    "b.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad.nn import Linear, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.uniform((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[0.36063824])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(Tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[0.36063824]]), array([0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight, a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear as tLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tLinear(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0912], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.8610]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2302], requires_grad=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight, a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad.nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vibegrad.nn.sequential.Sequential at 0x78230ce29300>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Linear(256, 300),\n",
    "    ReLU(),\n",
    "    Linear(300, 300),\n",
    "    ReLU(),\n",
    "    Linear(300, 10),\n",
    "    Sigmoid()\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(256, 300, bias=True) <vibegrad.nn.activations.ReLU object at 0x78230ce2a1d0>Linear(300, 300, bias=True) <vibegrad.nn.activations.ReLU object at 0x78230ce2ad40>Linear(300, 10, bias=True) <vibegrad.nn.activations.Sigmoid object at 0x78230ce2a200>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170410"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(-1, 1, size=(100, 256))\n",
    "y = np.random.choice([0, 1], size=(100, 10), p=[0.5, 0.5])\n",
    "X.shape\n",
    "X = Tensor(X)\n",
    "y = Tensor(y) \n",
    "y.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10), (100, 10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X)\n",
    "out.data.shape, y.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.99999999 1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.50478    0.5045781  0.50492245 0.50493403 0.50484436 0.50486404\n",
      "  0.50459781 0.50493749 0.50469422 0.50491557]\n",
      " [0.99998415 0.99998482 0.9999899  0.99998887 0.99998618 0.99998754\n",
      "  0.99997963 0.99998759 0.99997617 0.99998792]\n",
      " [0.51275556 0.51289545 0.51317659 0.51318522 0.51302364 0.51327754\n",
      "  0.51273618 0.51324202 0.51233495 0.51328406]\n",
      " [0.90374019 0.90433488 0.91115241 0.9096726  0.90567458 0.90785542\n",
      "  0.90005659 0.90777582 0.89531833 0.90756914]\n",
      " [0.99985578 0.9998579  0.99989664 0.99988839 0.99986845 0.99988163\n",
      "  0.99982682 0.9998809  0.99979243 0.99988012]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [0.99999951 0.99999952 0.99999973 0.99999968 0.99999959 0.99999964\n",
      "  0.99999935 0.99999965 0.99999914 0.99999964]\n",
      " [0.50957098 0.50947771 0.50979619 0.51003521 0.50935063 0.51004212\n",
      "  0.50926543 0.50975739 0.50924287 0.50962659]\n",
      " [0.99967114 0.99967968 0.99976245 0.9997405  0.99970053 0.99972861\n",
      "  0.99961561 0.9997253  0.99955688 0.99972431]\n",
      " [0.99999931 0.99999934 0.99999961 0.99999956 0.99999941 0.99999951\n",
      "  0.99999909 0.99999949 0.99999883 0.9999995 ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99999754 0.9999976  0.99999847 0.99999833 0.99999785 0.99999817\n",
      "  0.9999968  0.99999811 0.999996   0.99999816]\n",
      " [0.67083268 0.67252357 0.67752186 0.67608404 0.67391694 0.67496337\n",
      "  0.66739135 0.67483751 0.66601571 0.67458288]\n",
      " [0.8543635  0.85567465 0.86282904 0.86081721 0.85715915 0.85897527\n",
      "  0.85001807 0.85917607 0.84605353 0.85916756]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.548504   0.54819558 0.55067519 0.55008142 0.54902332 0.54968847\n",
      "  0.54706014 0.54977001 0.54632071 0.5494491 ]\n",
      " [0.57189877 0.57228747 0.57404235 0.57398037 0.57362903 0.57429758\n",
      "  0.56969005 0.5735999  0.56915239 0.57332061]\n",
      " [0.99999515 0.99999524 0.99999704 0.99999661 0.99999581 0.99999632\n",
      "  0.99999375 0.99999636 0.99999239 0.99999631]\n",
      " [0.9999996  0.9999996  0.99999978 0.99999974 0.99999965 0.99999971\n",
      "  0.99999945 0.9999997  0.9999993  0.99999971]\n",
      " [0.99551564 0.99552095 0.99636272 0.9961734  0.99575156 0.99595596\n",
      "  0.99493425 0.99594875 0.99450383 0.99595554]\n",
      " [0.5202334  0.52053097 0.52112757 0.52114966 0.52095764 0.52116755\n",
      "  0.52025963 0.52093491 0.51966807 0.52103858]\n",
      " [0.9999999  0.99999991 0.99999995 0.99999994 0.99999992 0.99999993\n",
      "  0.99999987 0.99999993 0.99999982 0.99999993]\n",
      " [0.51324112 0.51346611 0.5138136  0.51370228 0.51329418 0.51358291\n",
      "  0.5133783  0.51356903 0.51307021 0.51367473]\n",
      " [0.97812265 0.97884938 0.9811397  0.98049627 0.97909015 0.97999258\n",
      "  0.97649683 0.97986355 0.97450078 0.97995377]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.87006379 0.87176892 0.87825852 0.87659629 0.87213251 0.87397867\n",
      "  0.86597149 0.87415961 0.8627191  0.87433668]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.92592767 0.92604159 0.93242607 0.93049481 0.92741184 0.92811468\n",
      "  0.92147914 0.92942902 0.91812941 0.92867393]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.9999951  0.99999523 0.99999698 0.99999666 0.99999568 0.99999632\n",
      "  0.99999378 0.99999625 0.99999219 0.99999627]\n",
      " [0.99993279 0.99993374 0.99995343 0.99994937 0.99993793 0.99994567\n",
      "  0.99991785 0.99994499 0.99990314 0.99994575]\n",
      " [0.8345885  0.83547518 0.84317639 0.84011397 0.8382495  0.84004856\n",
      "  0.83014693 0.84041595 0.82675328 0.84000797]\n",
      " [0.99999999 0.99999999 0.99999999 0.99999999 0.99999999 0.99999999\n",
      "  0.99999998 0.99999999 0.99999998 0.99999999]\n",
      " [0.51117336 0.51099501 0.51154877 0.51153237 0.51124343 0.51129461\n",
      "  0.51070319 0.51139681 0.51062989 0.51158137]\n",
      " [0.70341923 0.70374406 0.71139222 0.70735515 0.70510213 0.70754816\n",
      "  0.69866032 0.70806393 0.69624399 0.70755893]\n",
      " [0.99999999 0.99999999 1.         1.         0.99999999 0.99999999\n",
      "  0.99999999 0.99999999 0.99999998 0.99999999]\n",
      " [0.72785203 0.72742039 0.73608296 0.73359373 0.73017049 0.73205265\n",
      "  0.72372881 0.73166578 0.72006607 0.73142695]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.9813153  0.9817012  0.98404423 0.98323601 0.98231883 0.98301786\n",
      "  0.97994134 0.98288513 0.97849305 0.98310602]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99998977 0.99999027 0.99999364 0.99999294 0.99999117 0.99999225\n",
      "  0.99998711 0.99999227 0.99998423 0.99999205]\n",
      " [0.77414805 0.7745368  0.78173898 0.78057702 0.77551024 0.77824306\n",
      "  0.76938164 0.77755253 0.76564176 0.77820792]\n",
      " [0.95294314 0.95285648 0.9578673  0.95666494 0.95487448 0.95611763\n",
      "  0.9502303  0.95568913 0.94777159 0.95575863]\n",
      " [0.99999986 0.99999987 0.99999993 0.99999991 0.99999988 0.9999999\n",
      "  0.99999981 0.9999999  0.99999975 0.9999999 ]\n",
      " [0.99999998 0.99999998 0.99999999 0.99999999 0.99999998 0.99999999\n",
      "  0.99999997 0.99999998 0.99999996 0.99999999]\n",
      " [0.99999933 0.99999934 0.99999962 0.99999958 0.99999943 0.99999951\n",
      "  0.99999912 0.99999951 0.99999885 0.99999951]\n",
      " [0.97743985 0.97751059 0.98030215 0.97974078 0.97822115 0.97925146\n",
      "  0.97542991 0.97899153 0.97395267 0.97921482]\n",
      " [0.9999778  0.99997852 0.99998513 0.99998407 0.99997999 0.99998272\n",
      "  0.99997276 0.99998239 0.99996625 0.99998201]\n",
      " [0.99864556 0.99864951 0.99894752 0.99888903 0.99874435 0.9988241\n",
      "  0.99843281 0.99882211 0.99826023 0.99883307]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.64702039 0.64819004 0.65231005 0.65232272 0.64829646 0.65123358\n",
      "  0.64559992 0.65068266 0.64201297 0.65118918]\n",
      " [0.98401204 0.98405399 0.98630755 0.98558481 0.98440432 0.98518555\n",
      "  0.98246906 0.98516335 0.98111873 0.98527758]\n",
      " [0.60763126 0.60865971 0.61210484 0.61107348 0.60960708 0.60944231\n",
      "  0.60461278 0.60968791 0.60395158 0.60963467]\n",
      " [0.55676542 0.55632129 0.55879908 0.55778967 0.55697451 0.5573428\n",
      "  0.55491084 0.55760429 0.5542393  0.55826895]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.78517383 0.78531947 0.79274327 0.79074479 0.78765375 0.7892631\n",
      "  0.77878129 0.79017483 0.77618449 0.78926355]\n",
      " [0.52803981 0.52806648 0.52874929 0.52884293 0.52836527 0.52834307\n",
      "  0.52723592 0.52912964 0.5271955  0.52874163]\n",
      " [0.97140396 0.97180752 0.97522684 0.97452171 0.97255452 0.97370304\n",
      "  0.96951985 0.97368725 0.96754033 0.97357423]\n",
      " [0.99934093 0.99934754 0.99950051 0.99947645 0.99938506 0.99944598\n",
      "  0.99923261 0.99944199 0.99913376 0.99943397]\n",
      " [0.98292518 0.98309665 0.98551828 0.98490544 0.98383798 0.98412118\n",
      "  0.98136123 0.98450859 0.98013615 0.9844015 ]\n",
      " [0.99567151 0.99572924 0.99648628 0.9963165  0.99589064 0.99618724\n",
      "  0.99513074 0.99616699 0.9947377  0.9961817 ]\n",
      " [0.51240946 0.51189154 0.51217795 0.51250988 0.51199415 0.51213797\n",
      "  0.51198538 0.51245748 0.51155062 0.51227142]\n",
      " [0.99938717 0.99939474 0.9995419  0.99950668 0.99944088 0.99948328\n",
      "  0.99928174 0.99947097 0.99919774 0.9994802 ]\n",
      " [0.93827436 0.9385267  0.94443184 0.94310381 0.94002019 0.94218802\n",
      "  0.93426832 0.94193934 0.9322437  0.94158967]\n",
      " [0.83754487 0.83724455 0.84509933 0.84378643 0.83945022 0.84158807\n",
      "  0.83225255 0.842246   0.82888523 0.84192507]\n",
      " [0.99948476 0.99949548 0.99961204 0.9995898  0.99953019 0.99956994\n",
      "  0.99939299 0.99956122 0.99932329 0.99956425]\n",
      " [0.9987252  0.998765   0.9990266  0.99895588 0.99881531 0.9989079\n",
      "  0.99853549 0.99890489 0.99837098 0.99889368]\n",
      " [0.50365173 0.50356393 0.50385675 0.50371107 0.50376821 0.50372808\n",
      "  0.50354926 0.50372579 0.50348896 0.50365639]\n",
      " [0.99994637 0.99994726 0.99996458 0.99995929 0.99995214 0.99995662\n",
      "  0.99993424 0.99995693 0.99992129 0.99995601]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99801322 0.99800955 0.99849023 0.99834767 0.99813218 0.99825989\n",
      "  0.9977333  0.99826814 0.99746164 0.99825329]\n",
      " [0.52427783 0.52401061 0.52497123 0.52466545 0.52467971 0.52437665\n",
      "  0.52329499 0.52473573 0.52325998 0.52466308]\n",
      " [0.83807053 0.83877531 0.84648813 0.84508268 0.84049741 0.84270731\n",
      "  0.83247772 0.84283801 0.82932825 0.8425391 ]\n",
      " [0.94034548 0.94022564 0.94567813 0.94454032 0.94185356 0.94316295\n",
      "  0.93698774 0.94376888 0.93356618 0.9434146 ]\n",
      " [0.85917999 0.86071399 0.86818383 0.86594438 0.8621435  0.86354633\n",
      "  0.85386699 0.86477371 0.85174826 0.8636703 ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.6206189  0.62183501 0.6241613  0.62364616 0.62158374 0.62243342\n",
      "  0.61772993 0.62216005 0.61724906 0.62270738]\n",
      " [0.84318803 0.84262053 0.85170826 0.84912307 0.84405903 0.84665578\n",
      "  0.83776181 0.84819154 0.83394145 0.84566335]\n",
      " [0.6250629  0.62532026 0.62927075 0.62776144 0.62610256 0.62843551\n",
      "  0.62289853 0.62787112 0.62041994 0.62824627]\n",
      " [0.99844932 0.99846927 0.99879816 0.99871161 0.99856754 0.99867576\n",
      "  0.99825045 0.99865487 0.9980227  0.99866064]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.96982058 0.96976848 0.97331345 0.97240405 0.9710699  0.97185623\n",
      "  0.96729527 0.97189702 0.96543302 0.97156702]\n",
      " [0.51493042 0.51538389 0.51575026 0.51607592 0.51513589 0.51548475\n",
      "  0.51480374 0.51530665 0.51473854 0.51583591]\n",
      " [0.58629028 0.58546284 0.58892213 0.58853464 0.58596317 0.58777451\n",
      "  0.58402107 0.58767665 0.58209315 0.58677153]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [0.5882543  0.58821798 0.5911185  0.59041703 0.5891578  0.59032181\n",
      "  0.5857285  0.58965396 0.58493891 0.5902654 ]\n",
      " [0.99930973 0.99930324 0.99947458 0.99943841 0.99934733 0.9994018\n",
      "  0.99918189 0.99940508 0.99907083 0.99939   ]\n",
      " [0.9544322  0.95420986 0.95955728 0.95837267 0.95535265 0.95731103\n",
      "  0.9515396  0.95733826 0.94911303 0.95722221]\n",
      " [0.96892568 0.96909546 0.97331359 0.97212491 0.96966565 0.97115557\n",
      "  0.96665164 0.97137165 0.96506195 0.97126874]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.56935326 0.56860827 0.5723869  0.57166205 0.56913258 0.57064807\n",
      "  0.56738886 0.57108296 0.56669688 0.57011401]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]])\n",
      "5.589571211568793\n"
     ]
    }
   ],
   "source": [
    "loss_fn = BCELoss(reduction=\"mean\")\n",
    "print(out)\n",
    "loss = loss_fn(out, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1299)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "out = torch.tensor(out.data)\n",
    "y = torch.tensor(y.data)\n",
    "loss_fn_torch = torch.nn.BCELoss()\n",
    "y = y.float()\n",
    "out = out.float()\n",
    "loss_fn_torch(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "       [1, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "       [1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
    "       [0, 1, 1, 0, 1, 1, 0, 0, 1, 0],\n",
    "       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0]])\n",
    "\n",
    "for ding in a.data:\n",
    "    for dong in ding:\n",
    "        if dong != 1 and dong != 0:\n",
    "            raise ValueError(\"ding dong\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2990011586691898\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, target)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "prediction = Tensor(np.array([0.9, 0.4, 0.7, 0.2]))\n",
    "target = Tensor(np.array([1, 0, 1, 0]))\n",
    "\n",
    "loss_fn = BCELoss()\n",
    "loss = loss_fn(prediction, target)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=256, out_features=300, bias=True)\n",
       "  (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.append(torch.nn.Linear(256, 300))\n",
    "model.append(torch.nn.Linear(300,300))\n",
    "model.append(torch.nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 170410\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=256, out_features=300, bias=True)\n",
       "  (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vibegrad.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=2.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.relu(Tensor(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
