{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(2)\n",
    "b = Tensor(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a*b\n",
    "c.backward()\n",
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 4.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad, a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.zero_grad()\n",
    "a.zero_grad()\n",
    "b.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad.nn import Linear, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Linear(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.uniform((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[0.0104803])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(Tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[0.0104803]]), array([0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight, a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear as tLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tLinear(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1977], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.9004]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2973], requires_grad=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weight, a.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vibegrad.nn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vibegrad.nn.sequential.Sequential at 0x70f16f359fc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Linear(256, 300),\n",
    "    ReLU(),\n",
    "    Linear(300, 300),\n",
    "    ReLU(),\n",
    "    Linear(300, 10),\n",
    "    Sigmoid()\n",
    "])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(256, 300, bias=True) <vibegrad.nn.activations.ReLU object at 0x70f16f35bd90>Linear(300, 300, bias=True) <vibegrad.nn.activations.ReLU object at 0x70f16f35a1d0>Linear(300, 10, bias=True) <vibegrad.nn.activations.Sigmoid object at 0x70f16f35b9d0>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170410"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(-1, 1, size=(100, 256))\n",
    "y = np.random.choice([0, 1], size=(100, 10), p=[0.5, 0.5])\n",
    "X.shape\n",
    "X = Tensor(X)\n",
    "y = Tensor(y) \n",
    "y.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10), (100, 10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(X)\n",
    "out.data.shape, y.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[0.52371102 0.52212771 0.52391284 0.52243536 0.52238915 0.52378274\n",
      "  0.52223559 0.52238869 0.52334069 0.52391554]\n",
      " [0.74021388 0.72805077 0.74175655 0.72998293 0.72517927 0.73907045\n",
      "  0.72589462 0.72904875 0.73542084 0.74051033]\n",
      " [0.72459405 0.71512509 0.72736076 0.7169332  0.71062387 0.72533446\n",
      "  0.71100673 0.715234   0.72317662 0.72634784]\n",
      " [0.99758474 0.99679201 0.99779424 0.99684705 0.99638575 0.99762022\n",
      "  0.99637045 0.99685512 0.99737367 0.99768255]\n",
      " [0.9997031  0.9995442  0.99973032 0.99957144 0.9994689  0.9997062\n",
      "  0.99948006 0.99956429 0.9996605  0.99971294]\n",
      " [0.99902275 0.99860037 0.9991117  0.99864446 0.99837228 0.99900944\n",
      "  0.99841368 0.99862442 0.99887627 0.9990477 ]\n",
      " [0.89751351 0.8856383  0.89912556 0.88727056 0.88091905 0.89704192\n",
      "  0.88215501 0.8869923  0.89373865 0.89730334]\n",
      " [0.99982323 0.9997218  0.99984168 0.99973662 0.99967246 0.99982638\n",
      "  0.99967714 0.99973606 0.9997948  0.99982937]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.84178922 0.82943241 0.8437436  0.83155766 0.82561701 0.84202612\n",
      "  0.82535367 0.8308862  0.83902413 0.84226208]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [0.99999761 0.99999521 0.99999795 0.99999573 0.99999381 0.99999759\n",
      "  0.99999397 0.99999542 0.99999692 0.99999769]\n",
      " [0.99982056 0.99972129 0.99984114 0.99973807 0.99967481 0.99982518\n",
      "  0.99968257 0.99973286 0.99979285 0.99982806]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99995703 0.9999266  0.99996239 0.99993261 0.99990994 0.99995611\n",
      "  0.99991349 0.99992809 0.99994942 0.99995831]\n",
      " [0.59390193 0.58787631 0.59448873 0.58857277 0.58637258 0.59291663\n",
      "  0.58632158 0.58786624 0.59159343 0.59318353]\n",
      " [0.69039125 0.68113583 0.69270639 0.68285865 0.67827997 0.69077513\n",
      "  0.67950905 0.68190084 0.68832588 0.69177392]\n",
      " [0.99999991 0.9999998  0.99999993 0.99999982 0.99999973 0.99999992\n",
      "  0.99999974 0.99999982 0.99999989 0.99999992]\n",
      " [0.55573352 0.55282216 0.55638725 0.55308522 0.55195691 0.55585238\n",
      "  0.55193334 0.55336631 0.55546372 0.55602602]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [0.59831523 0.5925586  0.59909471 0.5945646  0.59074372 0.59845398\n",
      "  0.5905692  0.59256142 0.59642452 0.59737319]\n",
      " [0.99999989 0.99999974 0.99999991 0.99999978 0.99999966 0.99999989\n",
      "  0.99999967 0.99999976 0.99999986 0.9999999 ]\n",
      " [0.99999643 0.99999308 0.999997   0.99999371 0.99999118 0.99999649\n",
      "  0.99999139 0.99999325 0.99999546 0.99999658]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.88925734 0.87899261 0.8929149  0.88040348 0.87492754 0.89015585\n",
      "  0.87489275 0.87970578 0.88590098 0.8908093 ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99999969 0.99999931 0.99999974 0.99999938 0.9999991  0.99999969\n",
      "  0.99999914 0.99999935 0.9999996  0.99999971]\n",
      " [0.96397686 0.95731388 0.96568871 0.95879915 0.95499638 0.9641278\n",
      "  0.95567975 0.9588039  0.96222059 0.96467389]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99997463 0.9999563  0.9999782  0.99996009 0.99994695 0.9999749\n",
      "  0.99994691 0.99995824 0.9999695  0.9999757 ]\n",
      " [0.56181269 0.55845405 0.56244126 0.55841608 0.55690264 0.5608766\n",
      "  0.55696545 0.55794678 0.5602503  0.5617233 ]\n",
      " [0.99914175 0.99874821 0.99922317 0.99882815 0.99859858 0.99913075\n",
      "  0.99860524 0.99878116 0.99902525 0.99916925]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99036201 0.98762047 0.99088584 0.98796504 0.98636703 0.99019062\n",
      "  0.98651521 0.98781774 0.98945872 0.99047653]\n",
      " [0.53019715 0.52840046 0.53094744 0.52917906 0.52830981 0.53020538\n",
      "  0.52845328 0.5292919  0.53012005 0.5302962 ]\n",
      " [0.57682261 0.57289826 0.57820845 0.57344897 0.57232308 0.57696363\n",
      "  0.57191686 0.573728   0.57597602 0.57760919]\n",
      " [0.99999979 0.99999954 0.99999983 0.99999959 0.9999994  0.99999979\n",
      "  0.99999941 0.99999957 0.99999974 0.99999981]\n",
      " [0.58218927 0.57863137 0.58415147 0.57987553 0.57692904 0.58360966\n",
      "  0.5763604  0.57970735 0.58147411 0.58328137]\n",
      " [0.56835215 0.56433256 0.56876986 0.5649939  0.56303634 0.56762815\n",
      "  0.56309291 0.56476107 0.56707286 0.56862835]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.8584072  0.84668875 0.86033367 0.84673937 0.84246476 0.85812996\n",
      "  0.8425686  0.84639345 0.85358564 0.85834372]\n",
      " [0.70347667 0.69353159 0.70541456 0.6942992  0.68879636 0.70340968\n",
      "  0.68967329 0.69522888 0.70063998 0.7039646 ]\n",
      " [1.         0.99999999 1.         1.         0.99999999 1.\n",
      "  0.99999999 1.         1.         1.        ]\n",
      " [0.53099383 0.52957512 0.53107935 0.52908643 0.52920784 0.53098999\n",
      "  0.52940934 0.53014946 0.53066449 0.53120124]\n",
      " [0.99964553 0.99946278 0.99967692 0.99948621 0.99936976 0.99964077\n",
      "  0.99937541 0.99946814 0.99958299 0.99965422]\n",
      " [0.68983415 0.68085288 0.69280031 0.68151942 0.67690376 0.6892675\n",
      "  0.67893439 0.68167971 0.68759401 0.69096805]\n",
      " [0.993992   0.99220631 0.99446256 0.99250309 0.9913951  0.99405255\n",
      "  0.99145866 0.99227759 0.99343344 0.99418007]\n",
      " [0.89853238 0.88804999 0.90172148 0.88897905 0.88422821 0.89807696\n",
      "  0.88427389 0.88898522 0.89483547 0.90050235]\n",
      " [0.74441114 0.73515909 0.74906687 0.73806113 0.7331858  0.74609336\n",
      "  0.73266986 0.73768011 0.7445193  0.7480118 ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99998124 0.99996693 0.99998383 0.99996936 0.99995967 0.99998148\n",
      "  0.99996012 0.99996888 0.99997797 0.99998208]\n",
      " [0.75289839 0.74034975 0.75518794 0.74388725 0.73674085 0.75216752\n",
      "  0.73898505 0.74224958 0.7497004  0.75227525]\n",
      " [0.55928553 0.55503593 0.55999594 0.55664845 0.55429767 0.55903271\n",
      "  0.55435127 0.55564627 0.55749439 0.55841689]\n",
      " [0.52200813 0.52086591 0.52232249 0.52086147 0.52039201 0.52222974\n",
      "  0.52056702 0.5207651  0.52166715 0.52185016]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.70596188 0.69642807 0.70807055 0.69830185 0.69408001 0.70665669\n",
      "  0.69269059 0.69774518 0.70346668 0.70773238]\n",
      " [0.98954288 0.98659337 0.99001829 0.98717343 0.98543931 0.98945045\n",
      "  0.9856725  0.98700104 0.98863513 0.98960262]\n",
      " [0.6847292  0.67638745 0.68658746 0.67738804 0.67438782 0.68501177\n",
      "  0.6727583  0.67833592 0.68230964 0.68618265]\n",
      " [0.9884942  0.98534086 0.98917672 0.98586505 0.98418847 0.98841599\n",
      "  0.98418147 0.98569333 0.98754514 0.988778  ]\n",
      " [0.96306897 0.95666469 0.96446992 0.95796885 0.95414729 0.96308114\n",
      "  0.95451985 0.9574012  0.96154814 0.96358695]\n",
      " [0.6746963  0.66630888 0.67673548 0.66711396 0.66294844 0.67553442\n",
      "  0.66273883 0.66773348 0.67214076 0.67588331]\n",
      " [0.99999985 0.99999966 0.99999988 0.9999997  0.99999955 0.99999985\n",
      "  0.99999956 0.99999968 0.99999981 0.99999986]\n",
      " [0.76925886 0.75562788 0.77152503 0.75954883 0.75405795 0.76773267\n",
      "  0.75277697 0.75850161 0.76477131 0.76872346]\n",
      " [0.99685278 0.99571664 0.99707687 0.99592055 0.99525817 0.99683797\n",
      "  0.99532968 0.9958184  0.99652203 0.99692526]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99995183 0.99991946 0.99995855 0.99992508 0.99990409 0.99995253\n",
      "  0.99990548 0.99992426 0.99994434 0.99995492]\n",
      " [0.99999999 0.99999998 1.         0.99999999 0.99999998 0.99999999\n",
      "  0.99999998 0.99999999 0.99999999 0.99999999]\n",
      " [1.         1.         1.         1.         0.99999999 1.\n",
      "  0.99999999 1.         1.         1.        ]\n",
      " [0.99383737 0.99200472 0.99430104 0.99223205 0.99100706 0.99383485\n",
      "  0.9913211  0.99200721 0.99329359 0.99399307]\n",
      " [0.93918304 0.93087241 0.9410917  0.93200737 0.92740332 0.93905773\n",
      "  0.92790473 0.93113867 0.93699817 0.93995216]\n",
      " [0.53566076 0.53371061 0.53663639 0.53454046 0.53354883 0.53616206\n",
      "  0.53314377 0.53413894 0.53502042 0.53573949]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.99690314 0.99573385 0.99706968 0.99595354 0.99520098 0.99692439\n",
      "  0.99530374 0.99584711 0.9965381  0.99692813]\n",
      " [0.5929556  0.5880609  0.59435727 0.58914915 0.58685021 0.59348083\n",
      "  0.58702385 0.58923498 0.59193703 0.59384369]\n",
      " [0.99996637 0.99994263 0.99997036 0.99994673 0.99993045 0.99996722\n",
      "  0.99993218 0.99994605 0.99996043 0.99996775]\n",
      " [0.99891302 0.99846434 0.99902212 0.99854752 0.99825351 0.99894404\n",
      "  0.99826338 0.99849153 0.99880665 0.99895914]\n",
      " [0.50040307 0.50037621 0.5004196  0.50038353 0.5003861  0.50039274\n",
      "  0.50037953 0.50038439 0.50040194 0.50040699]\n",
      " [0.87996623 0.86856613 0.88368586 0.87108299 0.86548572 0.88007629\n",
      "  0.86533362 0.8699769  0.87649471 0.88119111]\n",
      " [0.99126818 0.98907211 0.99188161 0.98940754 0.98790213 0.99142403\n",
      "  0.98808731 0.98901241 0.99070198 0.99153238]\n",
      " [0.99987649 0.99980203 0.99988865 0.99981594 0.99976401 0.9998782\n",
      "  0.99976516 0.9998075  0.99985837 0.99988024]\n",
      " [0.55518795 0.55302659 0.55620377 0.55346856 0.5523348  0.55581943\n",
      "  0.55191388 0.55425778 0.55575713 0.55661794]\n",
      " [0.94665985 0.93887453 0.94861901 0.93973472 0.93537936 0.94686509\n",
      "  0.93556926 0.93874106 0.94440833 0.94738789]\n",
      " [0.52820875 0.52594425 0.5284207  0.5264544  0.5260651  0.52804278\n",
      "  0.52586537 0.52643625 0.52724865 0.52769004]\n",
      " [0.91790826 0.90725971 0.92045892 0.9089421  0.90389861 0.91775013\n",
      "  0.90398247 0.90840782 0.91486526 0.91860286]\n",
      " [0.99994849 0.99991408 0.99995416 0.99991929 0.99989775 0.99994913\n",
      "  0.9998988  0.99991652 0.99993839 0.99995116]\n",
      " [1.         0.99999999 1.         0.99999999 0.99999998 1.\n",
      "  0.99999998 0.99999999 0.99999999 1.        ]\n",
      " [0.5        0.5        0.5        0.5        0.5        0.5\n",
      "  0.5        0.5        0.5        0.5       ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.72859337 0.71893004 0.73024566 0.72070278 0.71432092 0.72928543\n",
      "  0.71649848 0.71961864 0.72574487 0.7291408 ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.54803894 0.5448215  0.54841223 0.54516804 0.54418617 0.54812239\n",
      "  0.54469339 0.54523023 0.54700556 0.54799878]\n",
      " [0.75727511 0.7466411  0.76003247 0.74855544 0.74143056 0.75839433\n",
      "  0.74317877 0.74824892 0.75464588 0.75801164]\n",
      " [0.99993791 0.99989579 0.99994649 0.99990424 0.99987602 0.99993739\n",
      "  0.99987976 0.99990394 0.99992661 0.99994004]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.        ]\n",
      " [0.72717113 0.71653615 0.73048185 0.71822532 0.71318867 0.72612916\n",
      "  0.71380205 0.71665694 0.7223664  0.72800322]])\n",
      "5.094987075532469\n"
     ]
    }
   ],
   "source": [
    "loss_fn = BCELoss(reduction=\"mean\")\n",
    "print(out)\n",
    "loss = loss_fn(out, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.3998)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "out = torch.tensor(out.data)\n",
    "y = torch.tensor(y.data)\n",
    "loss_fn_torch = torch.nn.BCELoss()\n",
    "y = y.float()\n",
    "out = out.float()\n",
    "loss_fn_torch(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
    "       [1, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "       [1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
    "       [0, 1, 1, 0, 1, 1, 0, 0, 1, 0],\n",
    "       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0]])\n",
    "\n",
    "for ding in a.data:\n",
    "    for dong in ding:\n",
    "        if dong != 1 and dong != 0:\n",
    "            raise ValueError(\"ding dong\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2990011586691898\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, target)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "prediction = Tensor(np.array([0.9, 0.4, 0.7, 0.2]))\n",
    "target = Tensor(np.array([1, 0, 1, 0]))\n",
    "\n",
    "loss_fn = BCELoss()\n",
    "loss = loss_fn(prediction, target)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdouble)\n\u001b[1;32m      3\u001b[0m loss_fn_torch \u001b[38;5;241m=\u001b[39m BCELoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mloss_fn_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m loss_fn_torch\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/run/media/nuclea/Code/vibegrad/vibegrad/nn/_losses.py:36\u001b[0m, in \u001b[0;36mBCELoss.__call__\u001b[0;34m(self, pred, actual)\u001b[0m\n\u001b[1;32m     34\u001b[0m     bce_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(bce_loss\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     bce_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbce_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout \u001b[38;5;241m=\u001b[39m Tensor(bce_loss, (pred, actual), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbce_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bce_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/common/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/common/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n      didn't match because some of the keywords were incorrect: out, axis\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "prediction = torch.tensor(np.array([0.9, 0.4, 0.7, 0.2]))\n",
    "target = torch.tensor(np.array([1,0,1,0]), dtype=torch.double)\n",
    "loss_fn_torch = BCELoss()\n",
    "loss_fn_torch(prediction, target)\n",
    "loss_fn_torch.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=256, out_features=300, bias=True)\n",
       "  (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential()\n",
    "model.append(torch.nn.Linear(256, 300))\n",
    "model.append(torch.nn.Linear(300,300))\n",
    "model.append(torch.nn.Linear(300,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 170410\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Linear(in_features=256, out_features=300, bias=True)\n",
       "  (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vibegrad.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=2.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.relu(Tensor(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
